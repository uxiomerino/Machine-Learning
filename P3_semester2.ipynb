{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxecto 3: Aprendizaxe semisupervisado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga e preprocesado dos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uxiom\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\uxiom\\AppData\\Local\\Temp\\ipykernel_19112\\1290518901.py:30: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  x_train = np.vstack(map(lambda x: np.expand_dims(x, 0), x_train))\n",
      "C:\\Users\\uxiom\\AppData\\Local\\Temp\\ipykernel_19112\\1290518901.py:31: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  y_train = np.vstack(y_train)\n",
      "C:\\Users\\uxiom\\AppData\\Local\\Temp\\ipykernel_19112\\1290518901.py:33: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  x_test = np.vstack(map(lambda x: np.expand_dims(x, 0), x_test))\n",
      "C:\\Users\\uxiom\\AppData\\Local\\Temp\\ipykernel_19112\\1290518901.py:34: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  y_test = np.vstack(y_test)\n",
      "C:\\Users\\uxiom\\AppData\\Local\\Temp\\ipykernel_19112\\1290518901.py:36: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  unlabeled = np.vstack(map(lambda x: np.expand_dims(x, 0), unlabeled))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "train = tfds.load('stl10', split = 'train', as_supervised = True)\n",
    "test = tfds.load('stl10', split = 'test', as_supervised = True)\n",
    "unlabeled = tfds.load('stl10', split = 'unlabelled', as_supervised = True)\n",
    "\n",
    "train = train.map(lambda x, y: (tf.image.resize(x, (32, 32)), y)) \n",
    "test = test.map(lambda x, y: (tf.image.resize(x, (32, 32)), y))\n",
    "unlabeled = unlabeled.map(lambda x, y: (tf.image.resize(x, (32, 32)), y))\n",
    "\n",
    "x_train = train.map(lambda x, y: x/255)\n",
    "y_train = train.map(lambda x, y: y)\n",
    "\n",
    "x_test = test.map(lambda x, y: x/255)\n",
    "y_test = test.map(lambda x, y: y)\n",
    "\n",
    "unlabeled = unlabeled.map(lambda x, y: x/255)\n",
    "\n",
    "x_train = tfds.as_numpy(x_train)\n",
    "y_train = tfds.as_numpy(y_train)\n",
    "\n",
    "x_test = tfds.as_numpy(x_test)\n",
    "y_test = tfds.as_numpy(y_test)\n",
    "\n",
    "unlabeled = tfds.as_numpy(unlabeled)\n",
    "\n",
    "x_train = np.vstack(map(lambda x: np.expand_dims(x, 0), x_train))\n",
    "y_train = np.vstack(y_train)\n",
    "\n",
    "x_test = np.vstack(map(lambda x: np.expand_dims(x, 0), x_test))\n",
    "y_test = np.vstack(y_test)\n",
    "\n",
    "unlabeled = np.vstack(map(lambda x: np.expand_dims(x, 0), unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 5000 x_test: 8000 x_unlabeled: 100000\n"
     ]
    }
   ],
   "source": [
    "# Comprobacion do tamaño dos datos\n",
    "print(\"x_train:\", len(x_train), \"x_test:\", len(x_test), \"x_unlabeled:\", len(unlabeled))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación do modelo inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "for i in range(4):\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_155 (Conv2D)         (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_156 (Conv2D)         (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_157 (Conv2D)         (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_158 (Conv2D)         (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_159 (Conv2D)         (None, 22, 22, 32)        9248      \n",
      "                                                                 \n",
      " flatten_23 (Flatten)        (None, 15488)             0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 10)                154890    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192,778\n",
      "Trainable params: 192,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 20s 117ms/step - loss: 1.8750 - categorical_accuracy: 0.2960\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 1.5604 - categorical_accuracy: 0.4162\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 1.3973 - categorical_accuracy: 0.4800\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 18s 112ms/step - loss: 1.2600 - categorical_accuracy: 0.5350\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 18s 113ms/step - loss: 1.1156 - categorical_accuracy: 0.5908\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 0.9605 - categorical_accuracy: 0.6538\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 0.7545 - categorical_accuracy: 0.7318\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 18s 117ms/step - loss: 0.5771 - categorical_accuracy: 0.7950\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 18s 114ms/step - loss: 0.3650 - categorical_accuracy: 0.8722\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 0.2350 - categorical_accuracy: 0.9228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28279846f50>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 17ms/step - loss: 0.1333 - categorical_accuracy: 0.9620\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 3.2164 - categorical_accuracy: 0.4305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2164132595062256, 0.43050000071525574]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train, y_train)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué red has escogido? ¿Cómo la has entrenado?\n",
    "\n",
    "    A rede debía ser completamente convolucional, polo que o modelo leva 5 capas convolucionais, todas con 32 filtros e un kernel de tamaño (3, 3). Ademáis leva unha capa Flatten() previa á capa de saída Dense, con 10 neuronas para predecir as 10 clases posíbeis, con activación 'softmax'. A rede foi entrenada durante 10 épocas, con un batch_size de 32 e un optimizador Adam cunha taxa de aprendizaxe de 0.001. A función de pérdida foi CategoricalCrossentropy, e a métrica Accuracy.\n",
    " \n",
    "2. ¿Cuál es la precisión del modelo en entrenamiento? ¿Y en test?\n",
    "\n",
    "    A precisión do modelo en entrenamento é do 96.20%, e en test do 43.05%.\n",
    "\n",
    "3. ¿Qué conclusiones sacas de los resultados del punto anterior?\n",
    "\n",
    "    A diferenza entre a precisión de entrenamento e a de test é moi grande, polo que o modelo padece overfitting e sobreadestra. Isto pode ser debido a que o modelo non ten conxunto de validación nin capas de regularización que frenen este sobreadestramento, polo que o modelo aprende moi ben os datos de entrenamento, pero non xeneraliza ben para os datos de test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto aprendizaxe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_training_v2(model_func, x_train, y_train, unlabeled, thresh=0.8, train_epochs=3):\n",
    "    train_data = x_train.copy()\n",
    "    train_labels = y_train.copy()\n",
    "    labeled_weight = 1 # Asigna un peso maior aos datos etiquetados (certeza)\n",
    "    train_weight = np.array([labeled_weight]*len(train_labels))\n",
    "    for i in range(train_epochs):\n",
    "        lista_unlabeled = []\n",
    "        model = model_func\n",
    "        model.fit(train_data, train_labels, sample_weight=train_weight) # Usa sample_weight para asignar o peso\n",
    "        y_pred = model.predict(unlabeled)\n",
    "        y_class, y_value = y_pred.argmax(axis = 1), y_pred.max(axis = 1) # Escolle a clase segundo a confianza máis alta e devolve tamen esa confianza\n",
    "        # Recorre cada valor da tupla (unlabeled_data, y_class, y_value)\n",
    "        for i in range(len(unlabeled)):\n",
    "            if y_value[i] > thresh:\n",
    "                # Asigna como peso dos datos sen etiquetar a súa confianza \n",
    "                lista_unlabeled.append(i)\n",
    "        train_data = np.concatenate((train_data, unlabeled[lista_unlabeled]), axis = 0)\n",
    "        train_labels = np.concatenate((train_labels, tf.keras.utils.to_categorical(y_class[lista_unlabeled], 10)), axis=0)\n",
    "        train_weight = np.concatenate([train_weight, y_value[lista_unlabeled]], axis=0)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_st = self_training_v2(model, x_train, y_train, unlabeled, thresh=0.8, train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_st.evaluate(x_train, y_train)\n",
    "model_st.evaluate(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué parámetros has definido para el entrenamiento?\n",
    "\n",
    "    O modelo inicial é o mesmo ca o do apartado anterior, polo que os parámetros iniciais son os mesmos. Ademáis, ao incluir os datos non clasificados, especificamos o parámetro 'labeled_weight' para que o modelo dea máis importancia aos datos clasificados que aos non clasificados. Durante o adestramento, en 'train_weight' vanse ir gardando a confianza do modelo para cada dato non clasificado, para ponderalo con respecto aos etiquetados. En 'lista_unlabeled' almacenamos os índices dos datos non etiquetados para os que o modelo ten unha certeza coa clasificación superior ao umbral (0.8 neste caso), para engadilos ao conxunto de datos de adedstramento.\n",
    "\n",
    "2. ¿Cuál es la precisión del modelo en entrenamiento? ¿Y en test?\n",
    "\n",
    "    La precisión del modelo en entrenamiento es del 41.20%, y en test del 33.05%.\n",
    "\n",
    "3. ¿Se mejoran los resultados obtenidos en el ejercicio anterior?\n",
    "\n",
    "    Non, empeoran. Evítase o sobreaxuste pero non melloran os resultados.\n",
    "\n",
    "4. ¿Qué conclusiones sacas de los resultados del punto anterior?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaxe semisupervisado de tipo autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "\n",
    "        self.encoder = tf.keras.Sequential()\n",
    "        self.encoder.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "        for i in range(4):\n",
    "            self.encoder.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.encoder.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.decoder = tf.keras.Sequential()\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(32, (3, 3), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(16, (5, 5), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(16, (5, 5), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(8, (6, 6), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(3, (7, 7), activation='relu'))\n",
    "\n",
    "        self.autoencoder = tf.keras.Sequential([self.encoder, self.decoder])\n",
    "\n",
    "        self.autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "    def fit(self, X, y, epochs=5, batch_size=32):\n",
    "        print(self.autoencoder.summary())\n",
    "        self.autoencoder.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def get_encoded_data(self, X, batch_size = 32):\n",
    "        return self.encoder.predict(X, batch_size = batch_size)\n",
    "    \n",
    "    def _del_(self):\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "    def __init__(self):\n",
    "        self.classifier = tf.keras.Sequential()\n",
    "        self.classifier.add(tf.keras.layers.Flatten())\n",
    "        self.classifier.add(tf.keras.layers.Dense(25, activation='relu'))\n",
    "        self.classifier.add(tf.keras.layers.Dense(15, activation='relu'))\n",
    "        self.classifier.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        self.classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "    def fit(self, X, y, epochs=5, batch_size=32):\n",
    "        self.classifier.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    def predict(self, X, batch_size=32):\n",
    "        return self.classifier.predict(X, batch_size=batch_size)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clasificador.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.classifier.evaluate(X, y)\n",
    "    \n",
    "    def _del_(self):\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semisupervised_training(autoencoder, clasificador, x_train, y_train, unlabeled):\n",
    "    \n",
    "    autoencoder_data = np.concatenate((x_train, unlabeled), axis=0)\n",
    "    autoencoder.fit(autoencoder_data, autoencoder_data)\n",
    "\n",
    "    clasificador_data = autoencoder.get_encoded_data(x_train)\n",
    "    clasificador.fit(clasificador_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_62 (Sequential)  (None, 11, 11, 32)        37888     \n",
      "                                                                 \n",
      " sequential_63 (Sequential)  (None, 32, 32, 3)         34275     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,163\n",
      "Trainable params: 72,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "3282/3282 [==============================] - 606s 184ms/step - loss: 0.0243 - mse: 0.0243\n",
      "Epoch 2/5\n",
      "3282/3282 [==============================] - 586s 179ms/step - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 3/5\n",
      "3282/3282 [==============================] - 1265s 386ms/step - loss: 0.0164 - mse: 0.0164\n",
      "Epoch 4/5\n",
      "3282/3282 [==============================] - 722s 220ms/step - loss: 0.0146 - mse: 0.0146\n",
      "Epoch 5/5\n",
      "3282/3282 [==============================] - 793s 242ms/step - loss: 0.0134 - mse: 0.0134\n",
      "157/157 [==============================] - 5s 29ms/step\n",
      "Epoch 1/5\n",
      "157/157 [==============================] - 3s 5ms/step - loss: 2.2873 - categorical_accuracy: 0.1276\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.1642 - categorical_accuracy: 0.1690\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 2.1023 - categorical_accuracy: 0.1838\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.0573 - categorical_accuracy: 0.1816\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.0202 - categorical_accuracy: 0.1912\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder((32, 32, 3))\n",
    "clasificador = Clasificador()\n",
    "semisupervised_training(autoencoder, clasificador, x_train, y_train, unlabeled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 8s 30ms/step\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 2.0111 - categorical_accuracy: 0.1924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0111160278320312, 0.19237500429153442]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = autoencoder.get_encoded_data(x_test)\n",
    "clasificador.score(pred_data, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuál es la arquitectura del modelo? ¿Y sus hiperparámetros (epochs, batch size, optimizador, ...)?\n",
    "\n",
    "\n",
    "    A arquitetura do encoder é igual á do modelo inicial, mentres que para o decoder utilizamos 5 capas Conv2DTranspose, con filtros descendentes e un kernel que aumenta de tamaño. Foi adestrado con 5 epochs e tamaño de lote 32. \n",
    "    En canto ao clasificador, e un clasificador moi sinxelo de 3 capas Dense, rematando nunha capa de saída de 10 neuronas, con activación 'softmax'. Foi adestrado con 10 epochs e tamaño de lote 32.\n",
    "\n",
    "2. ¿Cuál es la precisión del modelo en entrenamiento? ¿Y en test?\n",
    "\n",
    "\n",
    "    En test, a precisión do modelo é do 19.24%.\n",
    "\n",
    "3. ¿Se mejoran los resultados obtenidos en los puntos anteriores?\n",
    "\n",
    "\n",
    "    Non, de feito empeóranse bastante. Entendemos que pode ser por utilizar uns epochs reducidos (para que se executara nun tempo razoábel) e por ter un clasificador sinxelo, que non é capaz de aprender ben os datos.\n",
    "\n",
    "4. ¿Qué conclusiones sacas de este apartado?\n",
    "\n",
    "\n",
    "    É difícil sacar conclusións ao ter tan malos resultados, xa que aínda que supoñemos que parte da culpa é do clasificador, non sabemos se o autoencoder está ben implementado ou non."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaxe de tipo autoencoder en 1 paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiClasificadorSemisupervisado:\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "\n",
    "        self.encoder = tf.keras.Sequential()\n",
    "        self.encoder.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "        for i in range(4):\n",
    "            self.encoder.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.encoder.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.decoder = tf.keras.Sequential()\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(32, (3, 3), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(16, (5, 5), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(16, (5, 5), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(8, (6, 6), activation='relu'))\n",
    "        self.decoder.add(tf.keras.layers.Conv2DTranspose(3, (7, 7), activation='relu'))\n",
    "\n",
    "        self.clasificador = tf.keras.Sequential()\n",
    "        self.clasificador.add(tf.keras.layers.Flatten())\n",
    "        self.clasificador.add(tf.keras.layers.Dense(15, activation='softmax'))\n",
    "        self.clasificador.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        self.autoencoder = tf.keras.Sequential([self.encoder, self.decoder])\n",
    "        self.clasif = self.clasificador\n",
    "\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        enc_out = self.autoencoder(inputs)\n",
    "        clasif_out = self.clasif(enc_out)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=[enc_out, clasif_out])\n",
    "\n",
    "        self.model.compile(optimizer='adam', loss = ['mse', 'categorical_crossentropy'], metrics = ['mse', 'categorical_accuracy'])\n",
    "\n",
    "    def fit(self, X, y, unlabeled = None, epochs=5, batch_size=32):\n",
    "        \n",
    "        if unlabeled is None:\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "        else:\n",
    "            X_train = np.concatenate((X, unlabeled))\n",
    "            y_train = np.concatenate((y, np.zeros((unlabeled.shape[0], 10))))\n",
    "            \n",
    "        y_train_autoencoder = X_train\n",
    "        y_train_clasificador = y_train\n",
    "        \n",
    "        self.model.fit(X_train, [y_train_autoencoder, y_train_clasificador], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(X)[1])\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)[1]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        self.model.evaluate(X, [X, y])\n",
    "\n",
    "    def __del__(self):\n",
    "        tf.keras.backend.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3282/3282 [==============================] - 643s 195ms/step - loss: 0.1356 - sequential_16_loss: 0.0254 - sequential_15_loss: 0.1102 - sequential_16_mse: 0.0254 - sequential_16_categorical_accuracy: 0.4783 - sequential_15_mse: 0.0141 - sequential_15_categorical_accuracy: 0.0048\n",
      "Epoch 2/5\n",
      "3282/3282 [==============================] - 601s 183ms/step - loss: 0.1282 - sequential_16_loss: 0.0180 - sequential_15_loss: 0.1102 - sequential_16_mse: 0.0180 - sequential_16_categorical_accuracy: 0.5141 - sequential_15_mse: 0.0140 - sequential_15_categorical_accuracy: 0.0048\n",
      "Epoch 3/5\n",
      "3282/3282 [==============================] - 578s 176ms/step - loss: 0.1253 - sequential_16_loss: 0.0151 - sequential_15_loss: 0.1101 - sequential_16_mse: 0.0151 - sequential_16_categorical_accuracy: 0.5979 - sequential_15_mse: 0.0140 - sequential_15_categorical_accuracy: 0.0048\n",
      "Epoch 4/5\n",
      "3282/3282 [==============================] - 569s 173ms/step - loss: 0.1238 - sequential_16_loss: 0.0137 - sequential_15_loss: 0.1101 - sequential_16_mse: 0.0137 - sequential_16_categorical_accuracy: 0.6204 - sequential_15_mse: 0.0140 - sequential_15_categorical_accuracy: 0.0048\n",
      "Epoch 5/5\n",
      "3282/3282 [==============================] - 574s 175ms/step - loss: 0.1232 - sequential_16_loss: 0.0131 - sequential_15_loss: 0.1101 - sequential_16_mse: 0.0131 - sequential_16_categorical_accuracy: 0.6264 - sequential_15_mse: 0.0140 - sequential_15_categorical_accuracy: 0.0048\n"
     ]
    }
   ],
   "source": [
    "autoencoder_clasificador = MiClasificadorSemisupervisado((32, 32, 3))\n",
    "autoencoder_clasificador.fit(x_train, y_train, unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 17s 67ms/step - loss: 2.3242 - sequential_16_loss: 0.0131 - sequential_15_loss: 2.3111 - sequential_16_mse: 0.0131 - sequential_16_categorical_accuracy: 0.6287 - sequential_15_mse: 0.0902 - sequential_15_categorical_accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "autoencoder_clasificador.score(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuál es la arquitectura del modelo? ¿Y sus hiperparámetros (epochs, batch size, optimizador, ...)?\n",
    "\n",
    "    A arquitetura do autoencoder é igual ca no anterior apartado. En canto ao clasificador, retiramos a primeira capa con respecto ao apartado anterior, quedando con dúas de 15 e 10 neuronas, respectivamente. Foi adestrado con 5 epochs e tamaño de lote 32.\n",
    "\n",
    "2. ¿Cuál es la precisión del modelo en entrenamiento? ¿Y en test?\n",
    "\n",
    "    Ao implementar varias métricas, xa que estamos adestrando nun paso un autoencoder e un clasificador, o resultado non é moi visual, pero a precisión do autoencoder correspóndese con sequential_16_categorical_accuracy e ten un valor de 62.87% en test. A precisión do clasificador é sequential_15_categorical_accuracy, e resultou dun 10% en test.\n",
    "\n",
    "3. ¿Se mejoran los resultados obtenidos en los puntos anteriores?\n",
    "\n",
    "    O accuracy final do clasificador empeora, pero ao observar por separado o do autoencoder, vemos que é bastante superior ao do clasificador. p\n",
    "\n",
    "4. ¿Qué conclusiones sacas de este apartado?\n",
    "\n",
    "    Confirmamos a nosa hipótese do apartado anterior de que a baixa precisión final debíase ao clasificador. O autoencoder está ben implementado, xa que aprende ben a representación latente dos datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
